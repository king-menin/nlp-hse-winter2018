{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word vectors.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Fs4J-wpVYkC-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Fun with Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "YOk10pekpZxe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtvdnKnUYkDA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Установка библиотек\n",
        "(For colab, or linux)"
      ]
    },
    {
      "metadata": {
        "id": "etYA1EdVYkDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade nltk gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGQJrw9Zk2f5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMc0WvW1YkDK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Скачиваем данные\n",
        "(для тех у кого linux), у кого windows скачайте файлы вручную и поместите в папку с проектом."
      ]
    },
    {
      "metadata": {
        "id": "SEvf__RnYkDK",
        "colab_type": "code",
        "collapsed": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -O p5.txt \"https://raw.githubusercontent.com/king-menin/nlp-hse-winter2018/master/lecture%202.%20word%20vectors/p5.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cYSHSYEWmJl-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec pretrained"
      ]
    },
    {
      "metadata": {
        "id": "zA1tWwEtYkDX",
        "colab_type": "code",
        "collapsed": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -O ru.tar.gz \"https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz?dl=1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZzEoQ4kZYkDc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xvzf ru.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nmMoyjElrkBi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "nltk russian sent tokenizer"
      ]
    },
    {
      "metadata": {
        "id": "wMs0pLnErfz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -O russian.pickle https://raw.githubusercontent.com/mhq/train_punkt/master/russian.pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KKH8-uCj6cM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 0. Чтение данных"
      ]
    },
    {
      "metadata": {
        "id": "zEUhjVBUj9JQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"p5.txt\") as file:\n",
        "  data = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X11K1AT7jtfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Токенизация"
      ]
    },
    {
      "metadata": {
        "id": "CIUYgwyDjs6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import nltk\n",
        "sent_tokenizer = nltk.data.load('russian.pickle')\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(sent_tokenizer.sentences_from_text(data[:1000])[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1DjeerGlkNvY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Задание:\n",
        "\n",
        "* Удалите пустые строки. Пока не убедитесь, что пустые строки не удалены, не запускайте ru_sent_tokenize.\n",
        "* приведите все к нижнему регистру и извлеките токены с помощью tokenizer. \n",
        "* data_tok должна содержать список токенов для каждого предложения в файле."
      ]
    },
    {
      "metadata": {
        "id": "R7t-_NYNpSJu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EC2Yqnjio31Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "data = <your code here>\n",
        "data_tok = <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CYqLCnmStTCX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test\n",
        "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "is_latin = lambda tok: all('а' <= x.lower() <= 'я' for x in tok)\n",
        "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_kxwjc3amdBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(data_tok[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aG4pINyYtqqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Word vectors\n",
        "Есть несколько способов обучения word embeddings. Word2Vec и GloVe с различными целевыми функциями. Или fasttext, который использует уровень символов.\n",
        "\n",
        "Выбор огромен, поэтому давайте начнем с малого, gensim еще одна библиотека nlp, которая включает в себя множество векторных моделей, в том числе word2vec."
      ]
    },
    {
      "metadata": {
        "id": "xNAjVTrftpke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(data_tok, \n",
        "                 size=32,      # embedding vector size\n",
        "                 min_count=1,  # consider words that occured at least 5 times\n",
        "                 window=5, iter=30).wv  # define context as a 5-word window around the target word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lTLZaHoHtp4_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Теперь можем получить вектор !\n",
        "model.get_vector('человек')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZ-VKQR0tp8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# или запросить похожие слова. Let's play!\n",
        "model.most_similar('человек')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3dRPKhvbyhSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Визуализация векторов слов\n",
        "Один из способов проверить, хороши ли наши векторы, - построить их. Дело в том, что эти векторы находятся в 3D + пространстве, и мы, люди, более привыкли к 2-3D.\n",
        "\n",
        "К счастью, мы, инжинеры, знаем о методах уменьшения размерности.\n"
      ]
    },
    {
      "metadata": {
        "id": "H9QGXA8X_BUW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Задание:\n",
        "Получите 2 списока слов: 50 ближайших к слову офицер и 50 ближайших к слову женщина и соедините в 1."
      ]
    },
    {
      "metadata": {
        "id": "ZBjfTIXTCYPY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words1 = <your code here>\n",
        "words2 = <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDLcgHdWD1t-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = words1 + words2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TpmlVEE2NWvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Получите вектора эмбеддингов"
      ]
    },
    {
      "metadata": {
        "id": "fP6FclzyCYUG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words1_vectors = <your code here>\n",
        "words2_vectors = <your code here>\n",
        "\n",
        "words_vectors = np.vstack((words2_vectors, words1_vectors))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pAQ0j33yGF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test\n",
        "assert isinstance(words_vectors, np.ndarray)\n",
        "assert words_vectors.shape == (100, 32)\n",
        "assert np.isfinite(word_vectors).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dECAvBwy1sXf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Linear projection: PCA\n",
        "Простейшим методом уменьшения размерности является Principial Component Analysis (PCA).\n",
        "\n",
        "В геометрических терминах PCA пытается найти оси, вдоль которых возникает большая часть дисперсии. «Естественные» оси, если хотите."
      ]
    },
    {
      "metadata": {
        "id": "-st-mM1h1-oG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://camo.githubusercontent.com/a68740188733509d867957a6b37712fa6253d2fd/68747470733a2f2f6769746875622e636f6d2f79616e646578646174617363686f6f6c2f50726163746963616c5f524c2f7261772f6d61737465722f7965745f616e6f746865725f7765656b2f5f7265736f757263652f7063615f666973682e706e67)"
      ]
    },
    {
      "metadata": {
        "id": "lYWWDZm-18MM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Under the hood, it attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing mean squared error:\n",
        "\n",
        "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
        "\n",
        "\n",
        "$X \\in \\mathbb{R}^{n \\times m}$ - object matrix (centered);\n",
        "\n",
        "$W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
        "\n",
        "$\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
        "\n",
        "$n$ samples, $m$ original dimensions and $d$ target dimensions;"
      ]
    },
    {
      "metadata": {
        "id": "8ulfDTrg4HjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Задание:\n",
        "Переведите вектора в 2d пространство с помощью PCA. Используйте \"good old\" sklearn api (fit, transform)\n",
        "После чего нормальчуйте вектора так, чтобы они имели mean=0 и variance=1"
      ]
    },
    {
      "metadata": {
        "id": "w2Ea3yiLyGDm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "word1_vectors_pca = <your code here>\n",
        "\n",
        "# Normalize\n",
        "word_vectors_pca = <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2-1vcyDyGBb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
        "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnwuFb745Cr5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Let's draw it!"
      ]
    },
    {
      "metadata": {
        "id": "Hm0elYjY5oZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%pylab inline\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "def draw_vectors(x_coords, y_coords, words=words):\n",
        "    # display scatter plot\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    plt.scatter(x_coords, y_coords, color=cm.rainbow([0.5] * 50 + [0.8] * 50))\n",
        "\n",
        "    for label, x, y in zip(words, x_coords, y_coords):\n",
        "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "    # plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
        "    # plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AfPbDVDB5OEP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], words=words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sepv7lZVE-Ub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Визуализируем с помощью t-SNE"
      ]
    },
    {
      "metadata": {
        "id": "hY7zytHIE5UV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "word_tsne = <your code here>\n",
        "\n",
        "# Normalize:\n",
        "word_tsne = <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_Kb_BckFK1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], words=words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLBDHtXpv9i9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Использование предобученной модели\n",
        "\n",
        "Это заняло время, а? Теперь представьте, что вы можете обучение эмбедднингов слов life-sized (100~300D) на гигабайты текста: статьи в Википедии или посты в Твиттере.\n",
        "К счастью, в настоящее время вы можете получить предварительно обученную модель эмбеддингов слов в 2 строки кода (без регистрации и смс)."
      ]
    },
    {
      "metadata": {
        "id": "j4FrbDp2tqAt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For en:\n",
        "# import gensim.downloader as api\n",
        "# model = api.load('glove-twitter-100')\n",
        "\n",
        "# For russian\n",
        "\n",
        "from gensim.models import *\n",
        "\n",
        "# ru.vec was downloaded in the top of notebook\n",
        "model300 = KeyedVectors.load_word2vec_format('ru.vec', binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sld3q9mZtqEF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model300.most_similar(positive=[\"человек\", \"офицер\"], negative=[\"генерал\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PryfW5TBF2U1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Задание:\n",
        "Повторите эксперимент и постройте TSNE график"
      ]
    },
    {
      "metadata": {
        "id": "FrXDLZPkF9wb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words1 = <your code here>\n",
        "words2 = <your code here>\n",
        "words = words1 + words2\n",
        "\n",
        "words1_vectors = <your code here>\n",
        "\n",
        "words2_vectors = <your code here>\n",
        "\n",
        "words_vectors = np.vstack((words2_vectors, words1_vectors))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LUFbhOCD5OAk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "word_tsne = <your code here>\n",
        "word_tsne = <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bHuxOqgu5N9c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], words=words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnqIK1vJYkFa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7. Анализ предложений\n",
        "#### Задание:\n",
        "1. lowercase phrase\n",
        "2. tokenize phrase\n",
        "3. average word vectors for all words in tokenized phrase\n",
        "\n",
        "\n",
        "* skip words that are not in model's vocabulary\n",
        "* if all words are missing from vocabulary, return zeros"
      ]
    },
    {
      "metadata": {
        "id": "Su_8ctmdH6Vd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_phrase_embedding(phrase):\n",
        "    \"\"\"\n",
        "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
        "    \"\"\"\n",
        "    vector = np.zeros([model300.vector_size], dtype='float32')\n",
        "    \n",
        "    <your code here>\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ScwiVqlOH0ba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vector = get_phrase_embedding(\" \".join(data_tok[0]))\n",
        "\n",
        "assert np.allclose(vector[:15],\n",
        "                   np.array([-0.09521323,  0.12494637,  0.07243977,  0.19425696,  0.09630267,\n",
        "                              0.02568892,  0.04206766, -0.22367822, -0.12968796,  0.10239005,\n",
        "                              0.01887055, -0.06602988, -0.07015807, -0.06562313, -0.11581734],\n",
        "                              dtype=np.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "avPfQDeQKC4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Задание:\n",
        "Визуализируйте 20 случайных фраз"
      ]
    },
    {
      "metadata": {
        "id": "ISEtWQjMJ91g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TwD4LueH0f-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# random choice\n",
        "chosen_phrases = <your code here>\n",
        "\n",
        "# compute vectors for chosen phrases\n",
        "phrase_vectors = <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OLSEQttvLX9P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test\n",
        "assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n",
        "assert phrase_vectors.shape == (len(chosen_phrases), model300.vector_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1gyec_AH0kJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "word_tsne = <your code here>\n",
        "\n",
        "# Normalize\n",
        "word_tsne = <your code here>\n",
        "\n",
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], words=chosen_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3SOCNUi-OvPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8. Расстояние между высказываниями\n",
        "В конце давайте напишем алгоритм, который позволит считать расстояния между высказываниями"
      ]
    },
    {
      "metadata": {
        "id": "J9EBO3EIOuw0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d39c8bf2-c677-4cac-bea8-8fb696d655af"
      },
      "cell_type": "code",
      "source": [
        "data_vectors = np.array([get_phrase_embedding(\" \".join(l)) for l in data_tok])"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yt8oA33yPHG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def cosine(matrix, vector):\n",
        "    dotted = matrix.dot(vector)\n",
        "    matrix_norms = np.linalg.norm(matrix, axis=1)\n",
        "    vector_norm = np.linalg.norm(vector)\n",
        "    matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
        "    neighbors = np.divide(dotted, matrix_vector_norms)\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def find_nearest(query, k=10):\n",
        "    \"\"\"\n",
        "    given text line (query), return k most similar lines from data, sorted from most to least similar\n",
        "    similarity should be measured as cosine between query and line embedding vectors\n",
        "    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n",
        "    \"\"\"\n",
        "    vector = get_phrase_embedding(query)\n",
        "    neighbors = <your code here>\n",
        "    ids = <your code here>\n",
        "    \n",
        "    return <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhAvh9o8PmNI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\" \".join(find_nearest(\" \".join(data_tok[0]), 1)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LGmHID3YkFb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 9. Word Mover's Distance\n",
        "#### по статье Matthew J. Kusner'а \"From Word Embeddings to Document Distances\" [1]\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/a15d483ec194469a9bea650fed26bf891b07ae94/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6d6b75736e65722f776d642f6d61737465722f666967312e706e67\">\n",
        "\n",
        "### Формулировка задачи определения сходства между двумя предложениями как задачи транспортной задачи:\n",
        "\n",
        "Пусть $X \\in \\mathbb{R}^{d \\times n}$ – матрица эмбеддингов,\n",
        "\n",
        "$d$ – размерность эмбеддинга, $n$ - количество слов;\n",
        "\n",
        "Вектор-документ в векторной модели: $d \\in \\mathbb{R}^n$ состоит из $c_i = \\texttt{count}(word_i, doc)$\n",
        "\n",
        "Нормированный вектор-документ: $d_i = \\frac{c_i}{\\sum_i c_i}$\n",
        "\n",
        "Расстояние между словами: $\\texttt{cost}(word_i, word_j) = ||x_i - x_j||_2$\n",
        "\n",
        "Дано два документа, $d, d'$. Пусть  $T \\in \\mathbb{R}^{n \\times n}$, $T_{ij} \\ge 0$ – матрица потока показывает расстояния от каждого слова $d$ до $d'$.\n",
        "\n",
        "Транспортная задача:\n",
        "\n",
        "$\\min_{T \\ge 0} \\sum_{i,j}^n T_{ij}\\texttt{cost}(word_i, word_j) $\n",
        "\n",
        "при условии:\n",
        "\n",
        "$\\sum_{j} T_{ij} = d_i$\n",
        "\n",
        "$\\sum_{i} T_{ij} = d'_j$.\n",
        "\n",
        "Задача решается средствами линейного программирования."
      ]
    },
    {
      "metadata": {
        "id": "0j_8G5b7YkFf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s1 = 'Ученые обнаружили ископаемую ящерицу с парой теменных глаз'\n",
        "s2 = 'У палеогеновой ящерицы нашли вторую пару глаз'\n",
        "s3 = 'Apple через два года откажется от процессоров Intel'\n",
        "\n",
        "distance = model300.wmdistance(s1, s2)\n",
        "print ('distance between s1 and s2 = %.4f' % distance)\n",
        "\n",
        "distance = model300.wmdistance(s1, s3)\n",
        "print ('distance between s1 and s3 = %.4f' % distance)\n",
        "\n",
        "distance = model300.wmdistance(s2, s3)\n",
        "print ('distance between s2 and s3 = %.4f' % distance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tJu04XZ1YkFl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model300.init_sims(replace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBaouYkVYkFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "distance = model300.wmdistance(s1, s2)\n",
        "print ('distance between s1 and s2 = %.4f' % distance)\n",
        "\n",
        "distance = model300.wmdistance(s1, s3)\n",
        "print ('distance between s1 and s3 = %.4f' % distance)\n",
        "\n",
        "distance = model300.wmdistance(s2, s3)\n",
        "print ('distance between s2 and s3 = %.4f' % distance)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}